---
title: "Staying On Track: Investigating the Effects of Email Reminders on Combatting Procrastination"
author: "Karrie Chou"
date: "21/12/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(knitr)
library(tinytex)
library(survey)
```
```{r, include=FALSE}
setwd("~/Desktop/Coursework/STA304/Final Project")

merged_prototype <- read.csv("merged_prototype.csv")
dependent_variables <- read.csv("dependent_variables.csv")

merged_prototype_students <- 
  merged_prototype %>%
  filter(ca_consent == "True")
```

Code and data supporting this statistical analysis can be found at https://github.com/karriechou/staying-on-track. 

# Keywords
A/B comparisons, email, procrastination, online classroom, reminders. 

# Introduction
With many post-secondary institutions incorporating virtual tools such as online homework modules and asynchronous lectures, the average classroom setting has evolved over the past two decades to accommodate the lifestyle changes following the advent of the digital age. The COVID-19 pandemic has presented an obstacle that the educational world had not encountered before: a mass shift to online classrooms, where all teaching and evaulation happens in a virtual shell that acts as a placeholder for the physical classroom. In such environments, the challenge of how to better motivate students to engage with course material arises. While it is common for teaching staff to make use of emails and other online communication tools to remind students to complete homework, there is still much work to be done in determining how these reminders should be structured in order to connect with students and encourage them to complete their assessments in a timely manner. It is important for this research to be conducted, as procrastination is an age-old problem among students and can lead to frustration, stress, and poor performance in classes. 

To answer design-based questions such as the ones that come up when discussing how to best combat procrastination through sending email reminders, A/B comparison tests are a popular statistically-based approach. A/B testing, otherwise known as multivariate testing, was developed by Ronald Fisher in the 1920s, with its most current form having come into existence in the 1990s (Gallo, 2017). A/B testing involves using a randomized controlled experiment to compare the results of two different versions of a test. This paper describes a set of A/B comparisons that were conducted within an introductory-level computer science course at a post-secondary institution (Anonymous Authors, 2021). Different versions of email reminders to complete weekly homework sets were deployed across different times throughout the week, and differences in completion time and homework attempt rates were analyzed to determine which type of email reminder best combatted procrastination (Anonymous Authors, 2021). 

In the Methodology section, an overview of the dataset used to conduct the subsequent statistical analysis, as well as a description of the final regression model which quantifies the significance of certain design factors within a reminder email which impacted if a student attempted the homework at all, is provided. Results of the analysis will be discussed and placed into context in the Results section. Conclusions and future directions of this research will be explored within the Discussion section. 

# Methodology

## Data
The popluation from which this data was collected was students enrolled in an introductory computer science course at an accredited post-secondary research institution during the Winter 2020 academic semester. The data used for this analysis comes from two sources: 

- An online homework system (full name redacted) on which students complete weekly graded homework modules that were due on the Monday of the following week.
- A survey sent out after the study was concluded to all students who received reminder emails which asked for their feedback on the emails, their design, and their impact on the students' homework completion habits. 

Within the online homework module, students were allowed to make multiple submissions, with the highest scoring submission being their final grade on the assignment. Each submission from a unique student was given a randomized ID number, and because students used the same login information to answer the survey and complete their online homework modules, these IDs could be matched up across the two data sources to form a complete set of observations for each student. 

To make the dataset for the regression model and the subsequent data analysis in this report, a dataset containing information on how each student was divided into control and the various treatment groups as well as their survey answers (referred to henceforth as *merged_prototype*), and another dataset containing information on when students started and completed various homework attempts (referred to henceforth as *dependent_variables*) were combined using the following process: 

- Within merged_prototype, there is a variable which records if a student consented to have their homework and survey data used for research analysis. Using the tidyverse filter() function, only students who consented to having their homework and survey data used for research analysis were included in the model dataset. 
- Then, an inner join between dependent_variables and the filtered merged_prototype was carried out in order to match student IDs across the two datasets. 

Of particular concern to this data analysis is the data from the homework submissions for the 7^th^ week of the course. From here, items such as homework start and homework end times, homework attempt rates, homework completion rates, and the sizes for the control group and each treatment group were determined. For this study, each treatment group's size was as follows: 

- 437 students in the control group, receiving no reminder emails. 
- 561 students in the treatment group, receiving reminder emails. 

Within the treatment group, the following divisions were made to analyze the effects of the timing of the reminder emails: 

- 185 students were sent emails on Wednesday afternoon. 
- 196 students were sent emails on Wednesday evening. 
- 180 students were sent emails on Thursday afternoon. 

## Models

### General model description
This report will be analyzing four different models: two concerning homework attempt rate, and two concerning homework completion rate. For each response variable, the following two logistic regression models will be analyzed: 

- One model with a binary variable indicating whether or not a student received a reminder email as the explanatory variable. 
- One model with three separate binary variables indicating the timing of when a treatment group student received a reminder email as the explanatory variables. 

All four of the models will be logistic regression models. This is because the two dependent variables of interest are all binary variabbles. For a general logistic regression model, the mathematical expression is: 

$log(\frac{p}{1-p}) = \beta_{0} + \beta_{1}X_{1} + \beta_{2}X_{2}... + \beta_{k}X_{k}$

The model's left-hand side represents a binary response variable with a probability p of having a "successful" outcome. In any of the four proposed models, the response variable in question will either be a binary variable that dictates whether a student attempted the homework or a binary variable that dictates whether a student completed the homework. 

### Variables  of interest
The variables used in the calculation of each of the four estimated regression models were created by altering existing variables in the dataset using the Tidyverse mutate() function. All of these newly constructed variables are binary dummy variables in order to facilitate the creation of non-factor-based logistic regression models. These variables are under the following names: 

- w7_attempted_num: A numerical representation of the original factor variable w7_started, which recorded if a student had attempted the homework regardless of if they finished it or not. This variable took a value of 1 if the student had attempted the homework; otherwise, it was equal to 0. 
- w7_completed_num: A numerical representation of the original factor variable w7_complete, which recorded if a student had completed all questions in the homework module. This variable took a value of 1 if the student had compelted the homework; otherwise, it was equal to 0. 
- w7_control_treat_num: A numerical representation of the factor variable w7_contral_treat_label, which recorded if a student had received a reminder email due to being part of a treatment group within the study. This variable took a value of 1 if the student was in a treatment group; otherwise, it was equal to 0. 
- wed_aft: A binary dummy variable which is equal to 1 if a student in a treatment group received their reminder email on Wednesday afternoon. 
- wed_eve: A binary dummy variable which is equal to 1 if a student in a treatment group received their reminder email on Wednesday evening 
- thurs_aft: A binary dummy variable which is equal to 1 if a student in a treatment group received their reminder email on Thursday afternoon. 

```{r, include=FALSE}
model_data <- inner_join(dependent_variables, merged_prototype_students, by = c("hashedid"))

model_data$w7_attempted_num = ifelse(model_data$w7_started == 'True', 1, 0)
model_data$w7_completed_num = ifelse(model_data$w7_complete == 'True', 1, 0)
model_data$w7_control_treat_num = ifelse(model_data$w7_contral_treat_label == 'Get email', 1, 0)
model_data$wed_aft = ifelse(model_data$w7_wed_email == 1 & model_data$w7_afternoon_email == 1, 1, 0)
model_data$wed_eve = ifelse(model_data$w7_wed_email == 1 & model_data$w7_afternoon_email == 0, 1, 0)
model_data$thurs_aft = ifelse(model_data$w7_wed_email == 0, 1, 0)

# Attempt rate models
attempt_model <- lm(w7_attempted_num ~ w7_control_treat_num, data = model_data, family = "binomial")
attempt_treatment_model <- lm(w7_attempted_num ~ wed_aft + wed_eve + thurs_aft, data = model_data, family = "binomial")

# Completion rate models
model_data %>%
  filter(w7_contral_treat_label == 'Get email')

completion_model <- lm(w7_completed_num ~ w7_control_treat_num, data = model_data, family = "binomial")
completion_treatment_model <- lm(w7_completed_num ~ wed_aft + wed_eve + thurs_aft, data = model_data, 
                                 family = "binomial")
```

# Results
Looking at the proportions of students who attempted the homework at all in this given week based on if they were in the control group (who did not receive any reminder emails) or the treatment group which received reminder emails at different times based on random selection, we can get a general understanding on if the reminder emails effectively encouraged students to attempt their homework. This will help us answer the question of if reminder emails should even be considered as a tool for mitigating student procrastination on assessments. 
```{r, include=FALSE}
graph_data <- inner_join(dependent_variables, merged_prototype_students, by = c("hashedid"))

control_count = 0
for (i in 1:nrow(graph_data)){
  if (graph_data$w7_contral_treat_label[i] == 'No email'){
    control_count = control_count + 1
  }
}

treatment_count = 0
for (i in 1:nrow(graph_data)){
  if (graph_data$w7_contral_treat_label[i] == 'Get email'){
    treatment_count = treatment_count + 1
  }
}

attempted <- 
  graph_data %>%
  filter(w7_started == 'True')

control_attempted = 0
for (i in 1:nrow(attempted)){
  if (attempted$w7_contral_treat_label[i] == 'No email'){
    control_attempted = control_attempted + 1
  }
}

treatment_attempted = 0
for (i in 1:nrow(attempted)){
  if (attempted$w7_contral_treat_label[i] == 'Get email'){
    treatment_attempted = treatment_attempted + 1
  }
}

plot_info <- data.frame(x_axis = c('Treatment', 'Control'), 
                        y_axis = c(treatment_attempted/treatment_count, control_attempted/control_count))
attempt_graph <- ggplot(data = plot_info, aes(x = x_axis, y = y_axis)) + 
  geom_bar(stat="identity", fill = 'steelblue', width = 0.4) + 
  labs(x = 'Condition', y = 'Probability of attempting homework', 
       caption = 'Figure 1: Proportion of students who attempted the homework based on treatment group')
```
```{r, echo=FALSE}
print(attempt_graph)
```
From the bar graph above, we see that $504/561 = 89.9\%$ of students in the treatment group attempted the homework, while $355/437 = 81.2\%$ of students in the control group attempted the homework. 

## A/B Comparisons of Start and End Times According to Treatment Group Allocation
```{r, include=FALSE}
table_1_data <- inner_join(dependent_variables, merged_prototype_students, by = c("hashedid"))
table_1_starters_data <- 
  table_1_data %>%
  filter(w7_started == 'True')

# Creating datasets: 
table_1_starters_email <- 
  table_1_starters_data %>%
  filter(w7_contral_treat_label == 'Get email')

table_1_starters_no_email <-
  table_1_starters_data %>%
  filter(w7_contral_treat_label == 'No email')

table_1_email <-
  table_1_data %>%
  mutate(w7_starttime_num = ifelse(is.na(w7_starttime), 0, w7_starttime), 
         w7_endtime_num = ifelse(is.na(w7_endtime), 0, w7_endtime)) %>%
  filter(w7_contral_treat_label == 'Get email')

table_1_no_email <- 
  table_1_data %>%
  mutate(w7_starttime_num = ifelse(is.na(w7_starttime), 0, w7_starttime), 
         w7_endtime_num = ifelse(is.na(w7_endtime), 0, w7_endtime)) %>%
  filter(w7_contral_treat_label == 'No email')

# Start times: 
## Forming individual tables to rbind: 
table_1_email_starttime <- 
  table_1_starters_email %>%
  summarize(mean = round(mean(w7_starttime), 2), sem = round(sqrt(var(w7_starttime)/nrow(table_1_starters_data)), 2), 
            n = nrow(table_1_starters_email))

table_1_no_email_starttime <-
  table_1_starters_no_email %>%
  summarize(mean = round(mean(w7_starttime), 2), sem = round(sqrt(var(w7_starttime)/nrow(table_1_starters_data)), 2), 
              n = nrow(table_1_starters_no_email))

table_1_email_starttime_nonstarters <-
  table_1_email %>%
  summarize(mean = round(mean(w7_starttime_num), 2), sem = round(sqrt(var(w7_starttime_num)/nrow(table_1_data)), 2), 
            n = nrow(table_1_email))

table_1_no_email_starttime_nonstarters <-
  table_1_no_email %>%
  summarize(mean = round(mean(w7_starttime_num), 2), sem = round(sqrt(var(w7_starttime_num)/nrow(table_1_data)), 2), 
            n = nrow(table_1_no_email))

## rbind-ing and c-binding: 
starttime <- rbind(table_1_email_starttime, table_1_no_email_starttime)
starttime_nonstarters <- rbind(table_1_email_starttime_nonstarters, table_1_no_email_starttime_nonstarters)

# End times: 
## Forming individual tables to rbind: 
table_1_email_endtime <- 
  table_1_starters_email %>%
  summarize(mean = round(mean(w7_endtime), 2), sem = round(sqrt(var(w7_endtime)/nrow(table_1_starters_data)), 2), 
            n = nrow(table_1_starters_email))

table_1_no_email_endtime <-
  table_1_starters_no_email %>%
  summarize(mean = round(mean(w7_endtime), 2), sem = round(sqrt(var(w7_endtime)/nrow(table_1_starters_data)), 2), 
            n = nrow(table_1_starters_no_email))

table_1_email_endtime_nonstarters <-
  table_1_email %>%
  summarize(mean = round(mean(w7_endtime_num), 2), sem = round(sqrt(var(w7_endtime_num)/nrow(table_1_data)), 2), 
            n = nrow(table_1_email))

table_1_no_email_endtime_nonstarters <-
  table_1_no_email %>%
  summarize(mean = round(mean(w7_endtime_num), 2), sem = round(sqrt(var(w7_endtime_num)/nrow(table_1_data)), 2), 
            n = nrow(table_1_no_email))

## rbind-ing and c-binding: 
endtime <- rbind(table_1_email_endtime, table_1_no_email_endtime)
endtime_nonstarters <- rbind(table_1_email_endtime_nonstarters, table_1_no_email_endtime_nonstarters)

# Final table: 
control_vs_treatment <- c('Email', "No email")

table_1_starttime <- cbind(control_vs_treatment, starttime)
table_1_starttime_nonstarters <- cbind(control_vs_treatment, starttime_nonstarters)
table_1_endtime <- cbind(control_vs_treatment, endtime)
table_1_endtime_nonstarters <- cbind(control_vs_treatment, endtime_nonstarters)
```
```{r, echo=FALSE}
kable(table_1_starttime, caption = "Start time means and standard errors for students who started the homework based on treatment group.")
```
```{r, echo=FALSE}
kable(table_1_endtime, caption = "End time means and standard errors for students who started the homework based on treatment group.")
```
Examining the data for start times between the two groups of students, we see that students who received an email prompting them to complete their homework actually started earlier -- there is a difference in start time means between the two groups of $46.03 - 45.74 = 0.29$ hours. 

It's important to note that the above calculation only takes into account students who started the homework at all. When we include "non-starter" students, or students who didn't attempt the homework on this given week, we obtain the following analysis: 

```{r, echo=FALSE}
kable(table_1_starttime_nonstarters, caption = "Start time means and standard errors for all students.")
```
```{r, echo=FALSE}
kable(table_1_endtime_nonstarters, caption = "End time means and standard errors for all students.")
```
Having lower means for both start and end times in this data analysis is understandable -- students who never started the homework were assigned start and end times of 0 hours in the original data set. However, the difference in start times between students who received an email and students who did not is still visible, with a difference in start time means between the two groups being $41.36 - 37.16 = 4.2$ hours. 

## A/B Comparisons of Start and End Times According to Email Groups
The previous section demonstrated that students who received any sort of email reminder did actually start their homework earlier on average. Now, this report will look at if there was any significant difference between homework start and end times based on the time at which a reminder email was sent to a student in the treatment group. 

```{r, include=FALSE}
table_3_data <- inner_join(dependent_variables, merged_prototype_students, by = c("hashedid"))
table_3_starters_data <- 
  table_3_data %>%
  filter(w7_started == 'True')

## Creating datasets: 
table_3_wed_aft_starters_data <-
  table_3_starters_data %>%
  filter(w7_wed_email == 1, w7_afternoon_email == 1)

table_3_wed_eve_starters_data <- 
  table_3_starters_data %>%
  filter(w7_wed_email == 1, w7_afternoon_email == 0)

table_3_thurs_aft_starters_data <-
  table_3_starters_data %>%
  filter(w7_wed_email == 0, w7_afternoon_email == 1)

table_3_wed_aft_data <-
  table_3_data %>%
  mutate(w7_starttime_num = ifelse(is.na(w7_starttime), 0, w7_starttime), 
         w7_endtime_num = ifelse(is.na(w7_endtime), 0, w7_endtime)) %>%
  filter(w7_wed_email == 1, w7_afternoon_email == 1)

table_3_wed_eve_data <- 
  table_3_data %>%
  mutate(w7_starttime_num = ifelse(is.na(w7_starttime), 0, w7_starttime), 
         w7_endtime_num = ifelse(is.na(w7_endtime), 0, w7_endtime)) %>%
  filter(w7_wed_email == 1, w7_afternoon_email == 0)

table_3_thurs_aft_data <-
  table_3_data %>%
  mutate(w7_starttime_num = ifelse(is.na(w7_starttime), 0, w7_starttime), 
         w7_endtime_num = ifelse(is.na(w7_endtime), 0, w7_endtime)) %>%
  filter(w7_wed_email == 0, w7_afternoon_email == 1)

# Start times: 
## Forming individual tables to rbind: 
table_3_wed_aft_starttime <- 
  table_3_wed_aft_starters_data %>%
  summarize(mean = round(mean(w7_starttime), 2), sem = round(sqrt(var(w7_starttime)/nrow(table_3_starters_data)), 2), 
            n = nrow(table_3_wed_aft_starters_data))

table_3_wed_eve_starttime <- 
  table_3_wed_eve_starters_data %>%
  summarize(mean = round(mean(w7_starttime), 2), sem = round(sqrt(var(w7_starttime)/nrow(table_3_starters_data)), 2), 
            n = nrow(table_3_wed_eve_starters_data))

table_3_thurs_aft_starttime <- 
  table_3_thurs_aft_starters_data %>%
  summarize(mean = round(mean(w7_starttime), 2), sem = round(sqrt(var(w7_starttime)/nrow(table_3_starters_data)), 2), 
            n = nrow(table_3_thurs_aft_starters_data))

table_3_wed_aft_starttime_nonstarters <- 
  table_3_wed_aft_data %>%
  summarize(mean = round(mean(w7_starttime_num), 2), sem = round(sqrt(var(w7_starttime_num)/nrow(table_3_data)), 2), 
            n = nrow(table_3_wed_aft_data))

table_3_wed_eve_starttime_nonstarters <- 
  table_3_wed_eve_data %>%
  summarize(mean = round(mean(w7_starttime_num), 2), sem = round(sqrt(var(w7_starttime_num)/nrow(table_3_data)), 2), 
            n = nrow(table_3_wed_eve_data))

table_3_thurs_aft_starttime_nonstarters <- 
  table_3_thurs_aft_data %>%
  summarize(mean = round(mean(w7_starttime_num), 2), sem = round(sqrt(var(w7_starttime_num)/nrow(table_3_data)), 2), 
            n = nrow(table_3_thurs_aft_data))

## rbind-ing and cbind-ing: 
final_starttime_3 <- rbind(table_3_wed_aft_starttime, table_3_wed_eve_starttime, table_3_thurs_aft_starttime)
final_starttime_3_nonstarters <- rbind(table_3_wed_aft_starttime_nonstarters, table_3_wed_eve_starttime_nonstarters,
                                       table_3_thurs_aft_starttime_nonstarters)

# End times: 
## Forming individual tables to rbind: 
table_3_wed_aft_endtime <- 
  table_3_wed_aft_starters_data %>%
  summarize(mean = round(mean(w7_endtime), 2), sem = round(sqrt(var(w7_endtime)/nrow(table_3_starters_data)), 2), 
            n = nrow(table_3_wed_aft_starters_data))

table_3_wed_eve_endtime <- 
  table_3_wed_eve_starters_data %>%
  summarize(mean = round(mean(w7_endtime), 2), sem = round(sqrt(var(w7_endtime)/nrow(table_3_starters_data)), 2), 
            n = nrow(table_3_wed_eve_starters_data))

table_3_thurs_aft_endtime <- 
  table_3_thurs_aft_starters_data %>%
  summarize(mean = round(mean(w7_endtime), 2), sem = round(sqrt(var(w7_endtime)/nrow(table_3_starters_data)), 2), 
            n = nrow(table_3_thurs_aft_starters_data))

table_3_wed_aft_endtime_nonstarters <- 
  table_3_wed_aft_data %>%
  summarize(mean = round(mean(w7_endtime_num), 2), sem = round(sqrt(var(w7_endtime_num)/nrow(table_3_data)), 2), 
            n = nrow(table_3_wed_aft_data))

table_3_wed_eve_endtime_nonstarters <- 
  table_3_wed_eve_data %>%
  summarize(mean = round(mean(w7_endtime_num), 2), sem = round(sqrt(var(w7_endtime_num)/nrow(table_3_data)), 2), 
            n = nrow(table_3_wed_eve_data))

table_3_thurs_aft_endtime_nonstarters <- 
  table_3_thurs_aft_data %>%
  summarize(mean = round(mean(w7_endtime_num), 2), sem = round(sqrt(var(w7_endtime_num)/nrow(table_3_data)), 2), 
            n = nrow(table_3_thurs_aft_data))

## rbind-ing and cbind-ing: 

final_endtime_3 <- rbind(table_3_wed_aft_endtime, table_3_wed_eve_endtime, table_3_thurs_aft_endtime)
final_endtime_3_nonstarters <- rbind(table_3_wed_aft_endtime_nonstarters, table_3_wed_eve_endtime_nonstarters, 
                                     table_3_thurs_aft_endtime_nonstarters)

# Final table: 
email_group <- c('Wednesday afternoon', "Wednesday evening", "Thursday afternoon")
table_3_starttime <- cbind(email_group, final_starttime_3)
table_3_starttime_nonstarters <- cbind(email_group, final_starttime_3_nonstarters)
table_3_endtime <- cbind(email_group, final_endtime_3)
table_3_endtime_nonstarters <- cbind(email_group, final_endtime_3_nonstarters)
```
```{r, echo=FALSE}
kable(table_3_starttime, caption = "Start time means and standard errors for treatment group students who started the homework based on email timing.")
kable(table_3_endtime, caption = "End time means and standard errors for treatment group students who started the homework based on email timing.")
```
From the above table detailing homework start times across the different treatment groups, emails that were sent closer to the due date of the homework typically saw students starting the homework later. This difference is so negligible among the different treatment groups that it can likely be attributed to the fact that students who were sent reminder emails who had not intended to do the homework earlier would have had less time to actually complete the homework. 

```{r, echo=FALSE}
kable(table_3_starttime_nonstarters, caption = "Start time means and standard errors for all treatment group students based on email timing.")
kable(table_3_endtime_nonstarters, caption = "End time means and standard errors for all treatment group students based on email timing.")
```
When we include "non-starter" students in the analysis of start and end times, as expected, the overall means of start and end times decrease. This same phenomenon was seen in the previous analysis of start and end times according to whether a student was placed in an email treatment group. 

## A/B Comparisons of Attempt and Completion Rates According to Email Groups
```{r, include=FALSE}
table_4_data <- inner_join(dependent_variables, merged_prototype_students, by = c("hashedid"))

# Creating datasets: 
table_4_wed_aft_data <-
  table_4_data %>%
  filter(w7_wed_email == 1, w7_afternoon_email == 1)

table_4_wed_eve_data <- 
  table_4_data %>%
  filter(w7_wed_email == 1, w7_afternoon_email == 0)

table_4_thurs_aft_data <-
  table_4_data %>%
  filter(w7_wed_email == 0, w7_afternoon_email == 1)

wed_aft_attempted_count = 0
for (i in 1:nrow(table_4_wed_aft_data)){
  if (table_4_wed_aft_data$w7_started[i] == 'True'){
    wed_aft_attempted_count = wed_aft_attempted_count + 1
  }
}
wed_aft_proportion_attempted = wed_aft_attempted_count/nrow(table_4_wed_aft_data)

wed_aft_completed_count = 0
for (i in 1:nrow(table_4_wed_aft_data)){
  if (table_4_wed_aft_data$w7_complete[i] == 'True'){
    wed_aft_completed_count = wed_aft_completed_count + 1
  }
}
wed_aft_proportion_completed = wed_aft_completed_count/nrow(table_4_wed_aft_data)

wed_aft_data <- 
  table_4_data %>%
  summarize(proportion_attempted = round(wed_aft_proportion_attempted, 2), 
            proportion_completed = round(wed_aft_proportion_completed, 2), 
            n = nrow(table_4_wed_aft_data))

wed_eve_attempted_count = 0
for (i in 1:nrow(table_4_wed_eve_data)){
  if (table_4_wed_eve_data$w7_started[i] == 'True'){
    wed_eve_attempted_count = wed_eve_attempted_count + 1
  }
}
wed_eve_proportion_attempted = wed_eve_attempted_count/nrow(table_4_wed_eve_data)

wed_eve_completed_count = 0
for (i in 1:nrow(table_4_wed_eve_data)){
  if (table_4_wed_eve_data$w7_complete[i] == 'True'){
    wed_eve_completed_count = wed_eve_completed_count + 1
  }
}
wed_eve_proportion_completed = wed_eve_completed_count/nrow(table_4_wed_eve_data)

wed_eve_data <- 
  table_4_data %>%
  summarize(proportion_attempted = round(wed_eve_proportion_attempted, 2), 
            proportion_completed = round(wed_eve_proportion_completed, 2), 
            n = nrow(table_4_wed_eve_data))

thurs_aft_attempted_count = 0
for (i in 1:nrow(table_4_thurs_aft_data)){
  if (table_4_thurs_aft_data$w7_started[i] == 'True'){
    thurs_aft_attempted_count = thurs_aft_attempted_count + 1
  }
}
thurs_aft_proportion_attempted = thurs_aft_attempted_count/nrow(table_4_thurs_aft_data)

thurs_aft_completed_count = 0
for (i in 1:nrow(table_4_thurs_aft_data)){
  if (table_4_thurs_aft_data$w7_complete[i] == 'True'){
    thurs_aft_completed_count = thurs_aft_completed_count + 1
  }
}
thurs_aft_proportion_completed = thurs_aft_completed_count/nrow(table_4_thurs_aft_data)

thurs_aft_data <- 
  table_4_data %>%
  summarize(proportion_attempted = round(thurs_aft_proportion_attempted, 2), 
            proportion_completed = round(thurs_aft_proportion_completed, 2), 
            n = nrow(table_4_thurs_aft_data))

# rbind-ing and cbind-ing: 
table_4_data <- rbind(wed_aft_data, wed_eve_data, thurs_aft_data)
table_4 <- cbind(email_group, table_4_data)
```
```{r, echo=FALSE}
kable(table_4, caption = "Attempt rates and completion rates across the three treatment groups.")
```
Again, there does not seem to be a significant difference in attempt rates across different treatment groups. However, completion rate was significantly higher for afternoon emails on both Wednesday and Thursday than evening emails. One possible explanation of this observation is the fact that in general, people tend to be more likely to work during the afternoon rather than in the evening or at night (Brueck, 2018). Therefore, by receiving a reminder in the afternoon, a student would be more inclined to access the homework module immediately after seeing the email. 

## Attempt Rate Model Regression Results
This section of the report will detail the findings from the four regression models proposed in the Methodology section. 
```{r, echo=FALSE}
kable(summary(attempt_model)$coefficients, caption = "Regression results for the attempt rate model that compares differences between control and treatment group students.")
```
Table 10 shows that the estimated logistic regression model for attempt rate on the condition that a student receives a reminder email is: $attempt\_rate = 0.8123 + 0.0863(X_1)$, where $X_1$ is a binary variable equal to 1 if a student receives a reminder and 0 otherwise. The most standard interpretation of this estimated model is that if a student receives a reminder email, they are 8.63% more likely to attempt (but not necessarily complete) the online homework module. 

This finding is statistically significant. Assuming a significance level of $\alpha = 0.05$, a p-value close to 0 indicates that there is a difference between the attempt rate for students who did not receive emails and students who did receive emails, meaning that the reminder emails had a statistically observable impact on encouraging students to start their homework. Such a finding is important to the study of how to combat procrastination in a learning environment, whether it be online or in-person, as it indicates that reminder emails are actually useful. 

```{r, echo=FALSE}
kable(summary(attempt_treatment_model)$coefficients, caption = "Regression results for the attempt rate model that compares differences across different treatment groups.")
```
Looking at the estimated regression model which takes into account differences between the treatment groups, there are remarkably less meaningful findings. The estimated model is: $attempt\_rate = 0.875 + 0.01689(wed\_aft) + 0.01276(wed\_eve)$, where wed_aft if equal to 1 if the student received a reminder email on Wednesday afternoon and wed_eve is equal to 1 if the student received it on Wednesday evening. By interpreting these estimated coefficients, we can see that a student receiving an email on Wednesday afternoon is 1.69% more likely to attempt the homework assignment, while a student receiving the same email on Thursday afternoon is only 1.28% more liiely to attempt the homework assignment. 

The dummy variable representing whether a student received an email on Thursday afternoon, which was included in the originally proposed model, was ommitted in the regression estimation due to singularities, meaning that it exhibited near perfect collinearity with another variable in the model. It is likely that this dummy variable was nearly perfectly collinear with wed_aft, as intuitively, productivity levels among students are likely to be similar around the same time on different dates. 

None of the estimated regression coefficients are significant at $\alpha = 0.05$ according to their p-values. Thus, there is no statistical evidence that the timing of a reminder email influences whether or not a student will attempt the homework assignment that needs to be finished. 

## Completion Rate Model Regression Results
```{r, echo=FALSE}
kable(summary(completion_model)$coefficients, caption = "Regression results for the completion rate model that compares differences between control and treatment group students.")
```
From Table 12, we can see that the estimated logistic regression model for the completion rate is $completion\_rate = 0.499 - 0.018(X_1)$, where X_1 is a binary variable that equals 1 if the student received a reminder email. While the model seems to say that a reminder email decreases the likelihood that a student completes their homework by 1.8%, it is important to note that the coefficient estimate is not statistically significant according to its p-value. Therefore, this regression model may not provide any relevant findings as to how sending reminder emails impacts homework completion rate. 

```{r, echo=FALSE}
kable(summary(completion_treatment_model)$coefficients, caption = "Regression results for the completion rate model that compares differences across different treatment groups.")
```
The same can be said for the estimated regression model which documents the impact of email timing on homework completion likelihood. While the estimated model shows that $completion\_rate = 0.538 - 0.003(X_1) - 0.135(X_2)$, with X_1 being equal to 1 if the email was sent on Wednesday afternoon and X_2 being equal to 1 if the email was sent on Wednesday evening, the fact that neither coefficient is statistically significant indicates that these estiamtes are meaningful to the overall study. The problem of the Thursday afternoon binary indicator variable being omitted due to singularities that was observed in Table 11 is also present here. 

# Discussion, future steps, and limitations
After creating a single dataset from two different sources and conducting preliminary analysis on summary variables which highlight the differences in start and end times between students who were in the control group and students who received a reminder email as part of the treatment group, four different regression models were investigated in order to determine the aspect of homework completion that the reminder emails had the greatest effect on. Out of the four proposed models, the one with the most statistical meaning is the one examining the connection between homework attempt rate and whether or not a student received an email reminder to attempt their homework. A plot of this model alongside the actual data is provided below. 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(model_data, aes(x=w7_control_treat_num, y=w7_attempted_num)) + 
  geom_point() + 
  geom_smooth(aes(x=w7_control_treat_num, w7_attempted_num), attempt_model) +
  labs(caption = "Figure 2: Plot of attempt rate regressed on student treatment group allocation.")
```
With the attempt rate being most significantly influenced by a student receiving a reminder email, it is evident that such reminders do combat procrastination in a meaningful way. While they don't necessarily impact the time at which a homework assignment is started, they certainly do encourage students to at least look at and try the homework questions. Such a result implies that perhaps future studies need to reframe the issue of procrastination -- instead of thinking of the problem as one of timing, it may be more productive to consider if a certain intervention makes students more aware of their upcoming deadlines and assessments. Additionally, future studies which take the research undertaken within this report into consideration can potentially explore the question of which features in particular, including email subject lines or content personalization, would entice students to use the email reminder as a way to manage their studying habits and attempt or even complete their homework on time. 

The main weakness of this study is that while there seems to be an overall significant increase in attempt rate among students who received reminder emails, it is difficult to determine the true impact of these emails because of the random nature by which students were allocated to control or treatment groups. This study and the methodology behind the data analysis in this report did not account for the fact that some students who were in either group may have been more organized and already completed the homework before any reminder email were sent out. As a result, these reminder emails would not have benefited this select group of students, leading to a potential misrepresentation of the effectiveness of reminder emails within this analysis. 

# References
Anonymous Author(s). 2021. Investigating the Impact of Online Homework Reminders Using Randomized A/B Comparisons. In SIGCSE ’21: SIGCSE Technical Symposium March 17–20, 2021, Toronto, Canada. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/1122445.1122456

Anonymous Author(s), (2021). [Investigating the Impact of Online Homework Reminders Using Randomized A/B Comparisons]. Unpublished raw data.

Brueck, H. (2018, August 13). The best time of day to do everything at work, according to science. Business Insider. https://www.businessinsider.com/best-time-day-work-according-to-science-2018-5. 

Gallo, A. (2017, June 28). A Refresher on A/B Testing. Harvard Business Review. https://hbr.org/2017/06/a-refresher-on-ab-testing. 
